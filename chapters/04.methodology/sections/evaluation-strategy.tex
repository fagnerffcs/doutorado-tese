\section{Evaluation Strategy: The Controlled Experiment}
\label{sec:evaluation}

The fifth step of the DSRM (Evaluation) is critical to demonstrate the utility and validity of the artifact. To achieve this, we employ a quantitative Controlled Experiment strategy, comparing the proposed artifact (PANOPTES) against a baseline (Vanilla Kubernetes).

\subsection{Experimental Method Characterization}
\label{sec:method-characterization}

To ensure scientific rigor and reproducibility, this research adopts the Controlled Experiment method. According to \citeonline{wohlin_2012}, experimentation in software engineering provides a systematic, disciplined, quantifiable, and controlled way of evaluating technical solutions. 

The design follows the framework established by \citeonline{basili_1986}, defined as follows:
\begin{itemize}
    \item \textbf{Goal:} Analyze the observability architectures.
    \item \textbf{Purpose:} Evaluate diagnostic efficiency and resource overhead.
    \item \textbf{Perspective:} Researcher and System Operator.
    \item \textbf{Context:} A controlled Kubernetes laboratory environment running a microservices workload under simulated load.
\end{itemize}

The choice of this method is justified by the need to isolate the ``Observability Architecture'' factor from confounding variables, ensuring that observed differences in metrics are indeed caused by the architecture and not by external noise \cite{wohlin_2012}.

\subsection{Research Hypotheses}
\label{sec:hypotheses}

Based on the research objectives, we formulate two primary hypotheses to test the artifact's effectiveness.

\textbf{Hypothesis 1: Diagnostic Efficiency}
We postulate that a unified observability architecture reduces the time required to diagnose anomalies.
\begin{itemize}
    \item $H_{0a}$: There is no significant difference in Mean Time to Detect (MTTD) between Vanilla Kubernetes and PANOPTES.
    \item $H_{1a}$: PANOPTES significantly reduces MTTD compared to Vanilla Kubernetes.
\end{itemize}

\textbf{Hypothesis 2: Resource Overhead}
We postulate that the resource consumption of the agents is negligible compared to the operational benefits.
\begin{itemize}
    \item $H_{0b}$: The resource overhead (CPU/Memory) introduced by PANOPTES exceeds 15\% of the total cluster capacity.
    \item $H_{1b}$: The resource overhead introduced by PANOPTES remains below 15\%, ensuring operational viability.
\end{itemize}

\subsection{Experimental Design}
\label{sec:experimental-design}

To test these hypotheses, we adopt a \textbf{2x2 Factorial Design}, crossing two independent variables: \textbf{Architecture Type} (Vanilla vs. PANOPTES) and \textbf{System State} (Idle vs. Chaos). This results in four distinct experimental scenarios, as detailed in Table \ref{tab:scenarios}.

\input{chapters/04.methodology/tables/scenarios}

The detailed configuration for each scenario is defined as follows:
\begin{itemize}
    \item \textbf{Scenario 1 (Baseline/Idle):} Standard Kubernetes without observability agents. Serves as the reference point for calculating overhead ($R_{base}$). Resource consumption is measured using the native \textbf{Kubernetes Metrics Server} to avoid observer effect.
    \item \textbf{Scenario 2 (PANOPTES/Idle):} Kubernetes with the full PANOPTES stack instantiated (OpenTelemetry, Prometheus, Loki, Tempo). Comparing S2 vs. S1 isolates the specific overhead introduced by the artifact.
    \item \textbf{Scenario 3 (Baseline/Stress):} Vanilla environment subjected to Chaos Engineering (e.g., Pod Kill). Diagnosis is attempted using only native CLI tools and unstructured logs.
    \item \textbf{Scenario 4 (PANOPTES/Stress):} PANOPTES environment subjected to the same faults. Diagnosis is performed using the proposed observability dashboards.
\end{itemize}

\subsection{Metrics and Measurement}
\label{sec:metrics}

To quantify the results, we utilize two categories of metrics, as defined in Table \ref{tab:metrics}.

\input{chapters/04.methodology/tables/metrics.tex}

\subsection{Instrumentation and Data Collection}
\label{sec:data-collection}

The data collection process is automated to ensure reproducibility:
\begin{itemize}
    \item \textbf{Instrumentation:} For the overhead analysis, resource metrics are collected via two distinct methods to avoid observer effect in the baseline. In Scenario 1 (Vanilla), we utilize the \textbf{Kubernetes Metrics Server} (native). In Scenario 2 (PANOPTES), we utilize the \textbf{kube-prometheus-stack}, which is part of the evaluated architecture.
    \item \textbf{Fault Injection:} \textbf{Chaos Mesh} is configured with distinct experiments (NetworkChaos, PodChaos) defined as YAML manifests.
    \item \textbf{Load Generation:} We utilize \textbf{k6} to simulate deterministic user traffic, ensuring the system is processing requests when faults occur.
\end{itemize}