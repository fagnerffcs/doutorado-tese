\section{Experimental Method Characterization}
\label{sec:method-characterization}

To ensure scientific rigor and reproducibility, this research adopts the Controlled Experiment method. According to \citeonline{wohlin_2012}, experimentation in software engineering provides a systematic, disciplined, quantifiable, and controlled way of evaluating human-based activities and technical solutions. Unlike case studies, which are observational and typically conducted in uncontrolled real-world settings, controlled experiments allow for the isolation of independent variables to determine causality with a higher degree of internal validity.

The design of this experiment follows the four-phase framework established by \citeonline{moher_schneider_1982} and refined by \citeonline{wohlin_2012}:

\begin{enumerate}
    \item \textbf{Definition (Scoping):} The objective is defined using the GQM (Goal-Question-Metric) approach, aiming to \textit{analyze} the observability architectures, for the \textit{purpose} of evaluation, with respect to \textit{diagnostic efficiency} and \textit{resource overhead}, from the point of view of the \textit{researcher}, in the context of a \textit{Kubernetes laboratory environment}.
    \item \textbf{Planning:} The design is determined (2x2 Factorial), variables are selected (Independent: Architecture Type; Dependent: MTTD, CPU/Memory), and subjects (simulated load/faults) are instrumented.
    \item \textbf{Operation:} The execution of the experiment in the testbed, collecting raw telemetry data across the defined scenarios.
    \item \textbf{Analysis and Interpretation:} The application of statistical tests to reject or accept the null hypotheses.
\end{enumerate}

\subsection{Suitability of the Method}

The choice of a Controlled Experiment is justified by the need to isolate the ``Observability Architecture'' factor from other confounding variables (such as network fluctuations or varying user traffic). As noted by \citeonline{basili_1986} and reinforced by \citeonline{wohlin_2012}, when the goal is to compare the efficacy of a specific technology (PANOPTES) against a baseline (Vanilla), control over the environment is paramount to ensure that observed differences in metrics (e.g., lower MTTD) are indeed caused by the architecture and not by external noise.