\chapter{OBSERVABILITY EXPERIMENT IMPLEMENTATION}
\label{chap:experiment-implementation}

This chapter details the technical implementation of the experimental design defined in the previous chapter. To satisfy the requirement for scientific reproducibility, the entire experimental testbed—from the virtualization layer to the application deployment—was codified using Infrastructure as Code (IaC) principles.

We describe the instantiation of the PANOPTES reference architecture, the configuration of the baseline environment (Vanilla), and the deployment of the target microservices workload used to generate telemetry data.

\section{Experimental Environment Implementation}
\label{sec:environment-implementation}

The experiments were conducted in a controlled laboratory environment designed to simulate a private cloud infrastructure. To mitigate the "it works on my machine" phenomenon and ensure that resource constraints (CPU/RAM) are strictly equal across all scenarios, we utilized Vagrant as the virtualization orchestrator.

\subsection{Virtualization Layer (Vagrant)}
The infrastructure consists of a Kubernetes cluster with one Control Plane node and two Worker Nodes. The specifications for each Virtual Machine (VM) are defined in a \texttt{Vagrantfile}, ensuring that both the Baseline and Experimental groups operate under identical hardware constraints.

\begin{table}[h]
\centering
\caption{Virtual Machine Specifications (Per Node)}
\label{tab:vm-specs}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Resource} & \textbf{Specification} & \textbf{Justification} \\ \hline
Hypervisor & VirtualBox 7.0 & Open-source Type-2 hypervisor. \\ \hline
OS & Ubuntu Server 22.04 LTS & Standard Linux kernel (5.15) with eBPF support. \\ \hline
vCPU & 2 Cores & Minimum for Kubernetes functional stability. \\ \hline
RAM & 4096 MiB & Sufficient for K8s + Observability Stack + Workload. \\ \hline
Network & Private Bridge & Isolated network to prevent external latency interference. \\ \hline
\end{tabular}
\end{table}

\section{Infrastructure as Code Strategy (Ansible)}
\label{sec:iac-strategy}

While Vagrant provisions the ``hardware'', the configuration of the software stack is managed by Ansible. We adopted a role-based architectural pattern to allow the dynamic switching between the Vanilla and PANOPTES scenarios using a simple feature flag mechanism.

The provisioning pipeline follows a deterministic sequence, as illustrated in Code Listing \ref{code:playbook}.

\begin{lstlisting}[language=yaml, caption={Main Playbook Structure}, label={code:playbook}]
- hosts: k8s_cluster
  roles:
    - { role: common, tags: ['base'] }
    - { role: container_runtime, tags: ['cri'] }
    - { role: kubernetes, tags: ['k8s'] }
    # Feature Flag controlling the experimental variable
    - { role: panoptes_stack, when: enable_observability | bool }
    - { role: chaos_mesh, when: enable_chaos | bool }
\end{lstlisting}

This approach guarantees that the *Vanilla* environment (Control Group) is mathematically a subset of the PANOPTES environment, minimizing confounding variables derived from manual installation differences.

\section{The PANOPTES Architecture Instantiation}
\label{sec:panoptes-instantiation}

The proposed reference architecture, PANOPTES, was implemented integrating the "Three Pillars" of observability into a cohesive stack. Unlike proprietary solutions that rely on "black-box" agents, PANOPTES is built entirely on CNCF open-source projects.

\subsection{Signal Generation and Collection (OpenTelemetry)}
We deployed the OpenTelemetry Collector in *DaemonSet* mode. This ensures that an instance of the collector runs on every worker node, capturing:
\begin{itemize}
    \item \textbf{Host Metrics:} CPU, Memory, and Disk I/O from the underlying Node.
    \item \textbf{Kubernetes Metrics:} Pod status and resource usage via the \textit{kubeletstats} receiver.
    \item \textbf{Application Traces:} OTLP (OpenTelemetry Protocol) spans generated by the microservices.
    \item \textbf{Logs:} Container logs parsed directly from \texttt{/var/log/pods}.
\end{itemize}

\subsection{Storage and Correlation Backends}
To process the high cardinality of data, the architecture routes signals to specialized backends:
\begin{enumerate}
    \item \textbf{Prometheus (Metrics):} Configured with a retention period of 15 days. It scrapes the OTel Collector for aggregated metrics.
    \item \textbf{Loki (Logs):} A label-based log aggregation system. Unlike Elasticsearch, Loki does not index the full text of logs, only the metadata (labels), making it highly resource-efficient for this architecture.
    \item \textbf{Tempo (Tracing):} A high-volume distributed tracing backend. It stores the full trace ID path, allowing the visualization of the request lifecycle across microservices.
\end{enumerate}

\subsection{Visualization Layer (Grafana)}
The "Single Pane of Glass" requirement is met by Grafana. We developed custom dashboards (provisioned as code) that automatically correlate these signals. For instance, clicking on a "High Latency" spike in a Prometheus graph automatically queries Tempo for the specific Trace IDs associated with that time window, bridging the gap between "knowing there is a problem" and "knowing where the problem is".

\section{Target Workload: Online Boutique}
\label{sec:workload}

To validate the diagnostic capabilities, we deployed the Google Online Boutique, a cloud-native microservices demo application. This workload was selected for three reasons:
\begin{enumerate}
    \item \textbf{Polyglot Nature:} It consists of 11 microservices written in Go, C\#, Node.js, Python, and Java, representing a realistic heterogeneous environment.
    \item \textbf{Communication Patterns:} It utilizes gRPC for internal service-to-service communication, which is notoriously difficult to debug without distributed tracing.
    \item \textbf{Reproducible Load:} It includes a load generator (\textit{Loadgenerator}) that simulates user traffic (browsing, adding to cart, checkout), ensuring the system is not idle during fault injection.
\end{enumerate}

\section{Fault Injection Implementation}
\label{sec:fault-injection-impl}

Consistent with the Chaos Engineering methodology defined in Chapter \ref{chap:cloud-computing}, we utilize Chaos Mesh to programmatically inject faults. The experiments defined in the Methodology \ref{chap:methodology} are implemented as Kubernetes Custom Resource Definitions (CRDs).

For example, the "Pod Failure" scenario is codified as follows:

\begin{lstlisting}[language=Yaml, caption={Pod Chaos Manifest}, label={code:pod-chaos}]
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: frontend-failure
spec:
  action: pod-kill
  mode: one
  selector:
    namespaces:
      - default
    labelSelectors:
      "app": "frontend"
  duration: "60s"
\end{lstlisting}

This declarative approach ensures that the "Stress" state is identical for both the Vanilla and PANOPTES scenarios.