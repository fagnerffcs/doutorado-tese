\subsection{The Pillars of Observability in Kubernetes}

Previously, it was noted that metrics, traces, and logs are pillars of observability. In K8s, these constructs are inconsistently defined and scattered throughout its architecture for other purposes, such as the Metrics Server, which is designed to enable autoscaling. However, before elaborating on finer details of K8s, it is necessary to know its current state.

\subsubsection{Metrics and the Prometheus Ecosystem} 
For numerical time-series data, Prometheus is the \textit{de facto} standard in the Cloud Native Computing Foundation (CNCF) ecosystem \cite{yeruva_2021}. The reference architecture uses the pull model, where the Prometheus server performs the scraping of metrics exposed by the `/metrics` endpoints of the services and Kubernetes itself (via kube-state-metrics) \cite{dubey_2021}.

For horizontal pod scalability (HPA - Horizontal Pod Autoscaler), the Metrics Server is an essential component that aggregates resource usage metrics and makes them available via the Kubernetes metrics API \cite{muschko_2020}.

\subsubsection{Logs and Centralized Aggregation}
In distributed environments, logs cannot be accessed in isolation via `kubectl logs' in production. The architecture requires a log forwarder (such as Fluentd or Fluent Bit) to centralize output streams. These logs must be processed to extract structured fields correlated with Kubernetes metadata before being sent to a storage backend (such as Elasticsearch, Loki, or OpenSearch) \cite{wilkins_2019,ibryam_2019}.

\subsubsection{Distributed Tracing}
To understand latency and request flow between microservices, distributed tracing is mandatory. Tools like Jaeger or Tempo receive spans generated by applications instrumented via OpenTelemetry. The OpenTelemetry Collector component plays a crucial role here, receiving traces via protocols such as OTLP, processing them (e.g., sampling and filtering sensitive data), and exporting them to the visualization backend \cite{hausenblas_2023,young_2024}.

\subsubsection{The Role of OpenTelemetry (OTel) in the Architecture}
OpenTelemetry (OTel) acts as the unifying layer in the reference architecture. It solves the problem of tool fragmentation by providing a single standard for APIs, SDKs, and protocols (OTLP) \cite{flanders_2024}.

A central component is the OpenTelemetry Collector, which can be deployed in two main modes in Kubernetes: Agent Mode (DaemonSet) or Gateway Mode (Deployment). In the first case, it runs on each node to collect logs and metrics from the host, minimizing network latency. In the second one, it is a centralized cluster of collectors that receives data from the agents, performs heavy processing (such as tail-based sampling and aggregation), and exports to the back.

OpenTelemetry has become so relevant that cloud vendors like Google have adopted the OTLP protocol to create their own telemetry API \cite{google_cloud_observability_2024}.